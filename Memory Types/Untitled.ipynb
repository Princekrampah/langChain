{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55ffb7c",
   "metadata": {},
   "source": [
    "## Memory Types In LangChain\n",
    "\n",
    "In this article we'll dive deep into the inner working of how chatbots can remember previous conversations in Langchain.\n",
    "\n",
    "In previuos articles, we have gone over the basic fundamental concepts in langchain. Chatbots need to remember previous conversations for human like conversation capabilities. Without this it will be so difficult to have a conversation with a chatbot. Imagine a talking to a person who can not remember the last thing you said, this will be the case with a chatbot that can not remember the things you said earlier on.\n",
    "\n",
    "The ability to store past information is what we refer to as **memory**. LangChain provides us with alot of utilities to add memory to our chains and chatbots. \n",
    "\n",
    "\n",
    "**A memory has two main functionalities, these are as follows:**\n",
    "\n",
    "1. **Read:** When chain recieves an input from user, it first fetches the previous conversation data from memory to make better sense of what the follow of the conversation is.\n",
    "\n",
    "2. **Write:** After the core logic of the chain is executed, involving input from LLM, the system then writes this to memory to keep track of what the AI's response was.\n",
    "\n",
    "![Image](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
    "[source](https://python.langchain.com/docs/modules/memory/)\n",
    "\n",
    "\n",
    "## Designing Memory Into A System\n",
    "\n",
    "There are two main things that we'll need to consider when integrating memory into a system:\n",
    "\n",
    "1. **How messages will be stored**\n",
    "2. **How messages will be retrieved**\n",
    "\n",
    "\n",
    "### Storage Of Messages\n",
    "\n",
    "In LangChain messages are stored in an in-memory database or a file(JSON) file specifically. \n",
    "\n",
    "### Retrieval Of Messages\n",
    "\n",
    "When it comes to message retrieval there can be a variety of complex ways around it. A simple way would be to return all the previous messages. A more complex system will return only K last messages. A more complicated system will return a summary of the whole previous conversations or a summary of the K last messages. Other approaches will return only previous messages that match a certain token length. We'll see memory types designed for some of these approaches. These are called **memory types**.\n",
    "\n",
    "\n",
    "### Memory Types\n",
    "\n",
    "There are a couple of memory types in Langchain that we can use. Here is a list of a few of them:\n",
    "\n",
    "1. ConversationBufferMemory\n",
    "2. ConversationBufferWindowMemory\n",
    "3. ConversationTokenBufferMemory\n",
    "4. ConversationSummaryMemory\n",
    "5. Knowledge Graph Memory\n",
    "6. Entity Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfa990",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "We'll use the code below to load and setup or environment variables. Make sure you have an OpenAI API key. I'll not go over how to do this as we covered this in the previous articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa736236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2273224",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key  = os.environ['OPANAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff142581",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "\n",
    "This is the most simplest form of memory. It simply stores all the previous chat data in a buffer and passes them to the prompt template when the logic is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf870cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d30760",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac461c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e487b",
   "metadata": {},
   "source": [
    "The above code returns the name of the variable that is retrieved from this kind of memory. Each memory type has its own variable name that is retrieved.\n",
    "\n",
    "``{}`` the empty dictionary is a playholder that we pass in when calling the ``load_memory_variables({})``\n",
    "\n",
    "We can also decide to name this memory variable ourselves. Here's how we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97230d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ''}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b61292",
   "metadata": {},
   "source": [
    "#### Adding data into the memory\n",
    "\n",
    "To add data, we can add data about what the **user input** was or what **the AI(LLM's) response** was. Here's how we do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab486ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"hello there!\")\n",
    "memory.chat_memory.add_ai_message(\"Hey, how can I assist you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "517ef21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human: hello there!\\nAI: Hey, how can I assist you today?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7906d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"Can you tell me more aboout XYZ\")\n",
    "memory.chat_memory.add_ai_message(\"Sure, here's what you need to know about XYZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c933043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: hello there!\\nAI: Hey, how can I assist you today?\\nHuman: Can you tell me more aboout XYZ\\nAI: Sure, here's what you need to know about XYZ\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84860c3",
   "metadata": {},
   "source": [
    "#### Returning Messages As A String Or A List Of Messages\n",
    "\n",
    "Messages can be returned as a string or a list of messages. It depends on where the messages are passed to. For LLMs, a simple string is best. For a ChatModel a list form is better. By default the messages are returned in form of a string. You can change this to get a list in return using ``return_message=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d17c3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2943e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"hello there!\")\n",
    "memory.chat_memory.add_ai_message(\"Hey, how can I assist you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2efe7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d234dfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}).get(\"history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54bc55",
   "metadata": {},
   "source": [
    "#### Keys Saved To Memory\n",
    " \n",
    "In some situations, chains (sequences of actions) can involve multiple inputs or outputs with different names. To manage this complexity, the system uses parameters called \"input_key\" and \"output_key\" associated with memory types. These parameters determine which specific input or output keys are stored in the chat message history. By default, these parameters are set to None, and if there's only one input/output key, it's automatically used. However, when there are multiple input/output keys, it's crucial to specify the particular key you want to use, ensuring clarity and control in managing the information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "973e9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc4a0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "template = \"\"\"You are a friendly AI that knows all about cats\n",
    "\n",
    "Previous conversations: {chat_history}\n",
    "\n",
    "Human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Here we need to align the memory_key to align with the one used in the prompt template string above\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41759442",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly AI that knows all about cats\n",
      "\n",
      "Previous conversations: \n",
      "\n",
      "Human question: Hello there, tell me the best cat food\n",
      "Response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello there, tell me the best cat food',\n",
       " 'history': '',\n",
       " 'text': \" Hi there! The best cat food really depends on your cat's individual needs. Generally, you want to look for a food that is high in protein, low in carbs, and free of fillers. Also, make sure it is specifically designed for cats, as their nutritional needs are different than those of other animals.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we just pass in the `question` variables the other placeholder `chat_history` gets populated by memory\n",
    "chain({\"question\": \"Hello there, tell me the best cat food\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f841599",
   "metadata": {},
   "source": [
    "#### Using ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d65b714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "442f2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a friendly AI that knows all about cats\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# `return_messages` is set to True to get a list of messages since we are using a ChatModel\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baee905f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a friendly AI that knows all about cats\n",
      "Human: Hello there, tell me the best cat food\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello there, tell me the best cat food',\n",
       " 'chat_history': [HumanMessage(content='Hello there, tell me the best cat food', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello! When it comes to choosing the best cat food, there are a few factors to consider. It\\'s important to look for a cat food that is nutritionally balanced and meets the specific needs of your cat. Here are some key things to consider:\\n\\n1. High-quality ingredients: Look for cat foods that list high-quality protein sources, such as chicken, turkey, or fish, as the main ingredient. Avoid foods that contain a lot of fillers, artificial preservatives, or by-products.\\n\\n2. Complete and balanced nutrition: Ensure that the cat food you choose is labeled as \"complete and balanced\" by the Association of American Feed Control Officials (AAFCO). This means it provides all the necessary nutrients for your cat\\'s life stage, whether it\\'s a kitten, adult, or senior cat.\\n\\n3. Life stage and specific needs: Consider your cat\\'s age, activity level, and any specific dietary requirements or health conditions they may have. Some cats may benefit from specialized diets, such as those formulated for weight management, urinary health, or sensitive stomachs.\\n\\n4. Wet or dry food: Both wet and dry cat food have their advantages. Wet food can provide additional hydration and can be easier for some cats to eat, while dry food can help maintain dental health. A combination of both can be a good option.\\n\\n5. Consult your veterinarian: Your veterinarian can provide valuable insights and recommendations based on your cat\\'s individual needs and health history.\\n\\nRemember, what works best for one cat may not work for another, so it\\'s important to find the right cat food that suits your cat\\'s preferences and nutritional needs.', additional_kwargs={}, example=False)],\n",
       " 'text': 'Hello! When it comes to choosing the best cat food, there are a few factors to consider. It\\'s important to look for a cat food that is nutritionally balanced and meets the specific needs of your cat. Here are some key things to consider:\\n\\n1. High-quality ingredients: Look for cat foods that list high-quality protein sources, such as chicken, turkey, or fish, as the main ingredient. Avoid foods that contain a lot of fillers, artificial preservatives, or by-products.\\n\\n2. Complete and balanced nutrition: Ensure that the cat food you choose is labeled as \"complete and balanced\" by the Association of American Feed Control Officials (AAFCO). This means it provides all the necessary nutrients for your cat\\'s life stage, whether it\\'s a kitten, adult, or senior cat.\\n\\n3. Life stage and specific needs: Consider your cat\\'s age, activity level, and any specific dietary requirements or health conditions they may have. Some cats may benefit from specialized diets, such as those formulated for weight management, urinary health, or sensitive stomachs.\\n\\n4. Wet or dry food: Both wet and dry cat food have their advantages. Wet food can provide additional hydration and can be easier for some cats to eat, while dry food can help maintain dental health. A combination of both can be a good option.\\n\\n5. Consult your veterinarian: Your veterinarian can provide valuable insights and recommendations based on your cat\\'s individual needs and health history.\\n\\nRemember, what works best for one cat may not work for another, so it\\'s important to find the right cat food that suits your cat\\'s preferences and nutritional needs.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we just pass in the `question` variables the other placeholder `chat_history` gets populated by memory\n",
    "chain({\"question\": \"Hello there, tell me the best cat food\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d1398",
   "metadata": {},
   "source": [
    "## Chat Messages\n",
    "\n",
    "The ChatMessageHistory class is a fundamental tool in memory modules, used across various scenarios. It's a simple and efficient wrapper that offers easy methods to save and retrieve both human and AI messages. If you're handling memory independently from a sequence of actions, you can directly employ this class for better management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fc45b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab30087d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi there', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hello, nice to hear from you', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Hi there\")\n",
    "history.add_ai_message(\"Hello, nice to hear from you\")\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e1d59",
   "metadata": {},
   "source": [
    "## Types Of Memory\n",
    "\n",
    "LangChain provides us with different types of memory classes. Each memory class has it's own usage and parameters. Let's explore these different types of memory we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851d475",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory\n",
    "\n",
    "This is used to store messages and then extract these messages in form of variables. We have taken a look at this type of memory so far. Lets make a closer look again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7553704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "090554e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09c175f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi there\\nAI: Hello, nice to hear from you'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbb3c2",
   "metadata": {},
   "source": [
    "We can also get a list form as mentioned earlier. This is suitable for chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66b9794c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi there', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello, nice to hear from you', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1d8cb",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd41e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d5219",
   "metadata": {},
   "source": [
    "### Conversation buffer window memory\n",
    "\n",
    "This is similar to the `conversation buffer` memory we looked as earlier on. The major difference being that the `window` name, meaning that only a list of the previous interactions are stored. Specifically the last `k` interactions. This helps to keep the memory buffer from going  large and also reduce token cost. Since we have less text being passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a70a1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f07dedce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Tell me about yourself\\nAI: Am an AI language model\\nHuman: Tell me about yourself, and what you enjoy doing\\nAI: Am an AI language model'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself, and what you enjoy doing\"}, {\"output\": \"Am an AI language model\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc954ace",
   "metadata": {},
   "source": [
    "You can see the output of the above code will only keep track of the last to interactions, this means the last two inputs and thier respectice ouputs.\n",
    "\n",
    "We can also get a list form as mentioned earlier. This is suitable for `chat models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d04a55de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Tell me about yourself', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Tell me about yourself, and what you enjoy doing', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself, and what you enjoy doing\"}, {\"output\": \"Am an AI language model\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd626962",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68556cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    # keep only the last 4 interactions in buffer memory\n",
    "    memory=ConversationBufferWindowMemory(k=4),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef79816",
   "metadata": {},
   "source": [
    "### Entity Memory\n",
    "\n",
    "`Entity Memory` is a system that retains provided facts about certain entities within a conversation. It gathers details about these entities using a large language model (LLM) and gradually enhances its understanding of them using the same LLM over time. Essentially, it collects and stores information about specific subjects within the conversation, using a language model to both extract and expand its knowledge about those subjects. Using this type of memory is more costly as additional LLM calls will have to be made to gather details on an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7cea14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4985dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationEntityMemory(llm=llm)\n",
    "\n",
    "_input = {\"input\": \"Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\"}\n",
    "\n",
    "memory.load_memory_variables(_input)\n",
    "\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" They must be the coolest kids in town\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de5d407e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\\nAI:  They must be the coolest kids in town\",\n",
       " 'entities': {'Johnson': 'Johnson is an older brother to Helen, adopted by her parents from different parents.'}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Johnson'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480671cc",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f00e2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ecb94766",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9584ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': '', 'Johnson': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's interesting. Can you tell me more about Helen and Johnson's relationship?\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "86bb2302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents.',\n",
       " 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents.\"}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ba78068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents.', 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents.\", 'pack': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "AI:  That's interesting. Can you tell me more about Helen and Johnson's relationship?\n",
      "Last line:\n",
      "Human: They are both at the pack playing football\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like fun! What position does each of them play?'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are both at the park playing football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "653780be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents, and is currently playing football at the park.', 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents. He is currently playing football at the park.\"}\n",
      "\n",
      "Current conversation:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "AI:  That's interesting. Can you tell me more about Helen and Johnson's relationship?\n",
      "Human: They are both at the pack playing football\n",
      "AI:  That sounds like fun! What position does each of them play?\n",
      "Last line:\n",
      "Human: They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! Do they have any other hobbies or interests they like to pursue together?\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "380690f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents, and is currently playing football at the park as a defender, while Johnson plays as a mid-fielder.', 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents. He is currently playing football at the park, with Helen playing as a defender and Johnson as a mid-fielder.\"}\n",
      "\n",
      "Current conversation:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "AI:  That's interesting. Can you tell me more about Helen and Johnson's relationship?\n",
      "Human: They are both at the pack playing football\n",
      "AI:  That sounds like fun! What position does each of them play?\n",
      "Human: They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\n",
      "AI:  That's great! Do they have any other hobbies or interests they like to pursue together?\n",
      "Last line:\n",
      "Human: Yah, they also like to ride thier bikes in thier free time together doing races\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like a lot of fun! Do they ever compete against each other?'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Yah, they also like to ride thier bikes in thier free time together doing races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f095d34",
   "metadata": {},
   "source": [
    "#### Inspecting Memory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff1c7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ad61fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted '\n",
      "          'by her parents, and is currently playing football at the park as a '\n",
      "          'defender, while Johnson plays as a mid-fielder. They also enjoy '\n",
      "          'riding bikes together in their free time and doing races.',\n",
      " 'Johnson': 'Johnson is an older brother to Helen, from different parents, and '\n",
      "            \"was adopted by Helen's parents. He is currently playing football \"\n",
      "            'at the park, with Helen playing as a defender and Johnson as a '\n",
      "            'mid-fielder, and they also enjoy riding their bikes together and '\n",
      "            'doing races in their free time.',\n",
      " 'pack': 'The pack is a location where Helen and Johnson play football.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0cc133fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Histon': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: They are both at the pack playing football\n",
      "AI:  That sounds like fun! What position does each of them play?\n",
      "Human: They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\n",
      "AI:  That's great! Do they have any other hobbies or interests they like to pursue together?\n",
      "Human: Yah, they also like to ride thier bikes in thier free time together doing races\n",
      "AI:  That sounds like a lot of fun! Do they ever compete against each other?\n",
      "Last line:\n",
      "Human: John works part-time in a company called Histon. Histon makes clothes for you children.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like a great job! What kind of clothes does Histon make for children?'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"John works part-time in a company called Histon. Histon makes clothes for you children.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7d644517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted '\n",
      "          'by her parents, and is currently playing football at the park as a '\n",
      "          'defender, while Johnson plays as a mid-fielder. They also enjoy '\n",
      "          'riding bikes together in their free time and doing races.',\n",
      " 'Histon': 'Histon is a company that makes clothes for children.',\n",
      " 'Johnson': 'Johnson is an older brother to Helen, from different parents, and '\n",
      "            \"was adopted by Helen's parents. He is currently playing football \"\n",
      "            'at the park, with Helen playing as a defender and Johnson as a '\n",
      "            'mid-fielder, and they also enjoy riding their bikes together and '\n",
      "            'doing races in their free time.',\n",
      " 'pack': 'The pack is a location where Helen and Johnson play football.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fcc16c",
   "metadata": {},
   "source": [
    "### Conversation Knowledge Graph Memory\n",
    "\n",
    "\n",
    "This memory variant employs a knowledge graph to reconstruct and represent memories. What's a knowledge graph?\n",
    "\n",
    "I aksed `ChatGPT` on what a knowledge graph is and this is the response I get back, `correct response` by the way ;)\n",
    "\n",
    "\"A knowledge graph is a structured representation of information that captures relationships between different concepts, entities, or data points. It's a way to organize and store data in a format that highlights how various pieces of information are connected to each other. Knowledge graphs consist of nodes (representing entities or concepts) and edges (representing relationships between nodes).\n",
    "\n",
    "In simpler terms, you can think of a knowledge graph as a map that shows not only individual locations (data points) but also the roads and paths (relationships) connecting those locations. This structure allows for more sophisticated and context-aware querying, reasoning, and analysis of data, making it easier to understand the connections and patterns within a dataset.\"\n",
    "\n",
    "**Basic terms**\n",
    "\n",
    "1. **Nodes ->** These are concepts, entitie or objects\n",
    "2. **Edges ->** These is the information used to create a relationship between the nodes\n",
    "\n",
    "**Semantic Enrichment**\n",
    "\n",
    "This is the use of NLP(Natural Language Processing) to identify nodes and create relationships between the nodes.\n",
    "\n",
    "**Example use cases**\n",
    "\n",
    "Semantic enrichment can be used with collected user data to come up with Knowledge Graphs that can better improve you recommendations on search engines and social media platforms\n",
    "\n",
    "For more infor on knowledge graphs, I'll recommend this [video](https://www.youtube.com/watch?v=PZBm7M0HGzw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f7c643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5f0caa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "\n",
    "memory.save_context({\"input\": \"Can you remind Jack to pick his sister from school\"}, {\"output\": \"What is the name of Jack's sister and what school is she at?\"})\n",
    "memory.save_context({\"input\": \"Jack's sister is called Helen and she studeis at Histon Primary School\"}, {\"output\": \"okay\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f2981d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'On Helen: Helen studies at Histon Primary School.\\nOn Jack: Jack has a sister name. Jack sister attends school.'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"Who is Helen\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36f9bd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack', 'Helen', 'Histon Primary School']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also more modularly get current entities from a new message (will use previous messages as context.)\n",
    "memory.get_current_entities(\"Jack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be82aa",
   "metadata": {},
   "source": [
    "### Getting Knowledge Triplets\n",
    "\n",
    "Knowledge triplets, often represented in the form of a knowledge graph, are a way to encode information about relationships between entities. A knowledge triplet consists of three main components: a subject, a predicate, and an object. This format is also referred to as \"Subject-Predicate-Object\" (SPO). Here's what each component means:\n",
    "\n",
    "**Subject ->** This is the entity about which the information is being stated. It's the main focus of the triplet.\n",
    "\n",
    "**Predicate ->** The predicate describes the relationship between the subject and the object. It's like a verb that connects the subject and object and explains the nature of their connection.\n",
    "\n",
    "**Object ->** The object is the entity to which the subject is related through the predicate. It completes the statement by specifying what the relationship means.\n",
    "\n",
    "For example, let's say we have the triplet \"Jack | loves | magoes.\" In this case:\n",
    "\n",
    "1. **Subject ->** Jack\n",
    "2. **Predicate ->** loves\n",
    "3. **Object ->** mangoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3ddfad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='Jack', predicate='loves', object_='mangoes')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_knowledge_triplets(\"Jack loves mangoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b972e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack', 'Helen', 'Histon Primary School']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_current_entities(\"Jack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e1484",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00ac53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "815e245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "# the order of the input_variables does not matter\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "chain_with_kg = ConversationChain(\n",
    "llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "15c5b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Jackson is the brother of Helen. Helen gets picked up daily from school by Jackson\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Ah, so Jackson is Helen's brother. That's nice that he takes the time to pick her up from school every day. Do you have any siblings, Human?\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_kg.predict(input=\"Jackson is the brother of Helen. Helen gets picked up daily from school by Jackson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a264c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Jackson: Jackson picks up Helen. Jackson is a character in story. Jackson is in love with woman named Helen. Jackson is very determined to win her heart. Jackson often puts himself in dangerous situations to help others. Jackson is a loyal friend and always has Helen's best interests in mind.\n",
      "On Helen: Helen gets picked up daily from school.\n",
      "\n",
      "Conversation:\n",
      "Human: What do you know about Helen\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I know that Helen gets picked up daily from school. I also know that she is the object of Jackson's affections and that he is very determined to win her heart. He often puts himself in dangerous situations to help others and is a loyal friend who always has her best interests in mind.\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_kg.predict(input=\"What do you know about Helen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "caa726c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Jackson: Jackson is in love with Helen. Jackson is a character in story. Jackson is in love with woman named Helen. Jackson is very determined to win her heart. Jackson often puts himself in dangerous situations to help others. Jackson is a loyal friend and always has Helen's best interests in mind. Jackson is very determined to win Helen's heart.\n",
      "On Helen: Helen gets picked up daily from school.\n",
      "\n",
      "Conversation:\n",
      "Human: What is the relationship between Jackson and Helen\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Jackson and Helen are in love. Jackson is very determined to win Helen's heart and often puts himself in dangerous situations to help others. He is a loyal friend and always has Helen's best interests in mind.\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_kg.predict(input=\"What is the relationship between Jackson and Helen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c1e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
