{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802ee9a0",
   "metadata": {},
   "source": [
    "## Memory Types In LangChain\n",
    "\n",
    "In this article we'll dive deep into the inner working of how chatbots can remember previous conversations in Langchain.\n",
    "\n",
    "In previuos articles, we have gone over the basic fundamental concepts in langchain. Chatbots need to remember previous conversations for human like conversation capabilities. Without this it will be so difficult to have a conversation with a chatbot. Imagine a talking to a person who can not remember the last thing you said, this will be the case with a chatbot that can not remember the things you said earlier on.\n",
    "\n",
    "The ability to store past information is what we refer to as **memory**. LangChain provides us with alot of utilities to add memory to our chains and chatbots. \n",
    "\n",
    "\n",
    "**A memory has two main functionalities, these are as follows:**\n",
    "\n",
    "1. **Read:** When chain recieves an input from user, it first fetches the previous conversation data from memory to make better sense of what the follow of the conversation is.\n",
    "\n",
    "2. **Write:** After the core logic of the chain is executed, involving input from LLM, the system then writes this to memory to keep track of what the AI's response was.\n",
    "\n",
    "![Image](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
    "[source](https://python.langchain.com/docs/modules/memory/)\n",
    "\n",
    "\n",
    "## Designing Memory Into A System\n",
    "\n",
    "There are two main things that we'll need to consider when integrating memory into a system:\n",
    "\n",
    "1. **How messages will be stored**\n",
    "2. **How messages will be retrieved**\n",
    "\n",
    "\n",
    "### Storage Of Messages\n",
    "\n",
    "In LangChain messages are stored in an in-memory database or a file(JSON) file specifically. \n",
    "\n",
    "### Retrieval Of Messages\n",
    "\n",
    "When it comes to message retrieval there can be a variety of complex ways around it. A simple way would be to return all the previous messages. A more complex system will return only K last messages. A more complicated system will return a summary of the whole previous conversations or a summary of the K last messages. Other approaches will return only previous messages that match a certain token length. We'll see memory types designed for some of these approaches. These are called **memory types**.\n",
    "\n",
    "\n",
    "### Memory Types\n",
    "\n",
    "There are a couple of memory types in Langchain that we can use. Here is a list of a few of them:\n",
    "\n",
    "1. ConversationBufferMemory\n",
    "2. ConversationBufferWindowMemory\n",
    "3. ConversationTokenBufferMemory\n",
    "4. ConversationSummaryMemory\n",
    "5. Knowledge Graph Memory\n",
    "6. Entity Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a07e65",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "We'll use the code below to load and setup or environment variables. Make sure you have an OpenAI API key. I'll not go over how to do this as we covered this in the previous articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff147dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key  = os.environ['OPANAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d4c16",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "\n",
    "This is the most simplest form of memory. It simply stores all the previous chat data in a buffer and passes them to the prompt template when the logic is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f842abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd82f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61ebc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030071f",
   "metadata": {},
   "source": [
    "The above code returns the name of the variable that is retrieved from this kind of memory. Each memory type has its own variable name that is retrieved.\n",
    "\n",
    "``{}`` the empty dictionary is a playholder that we pass in when calling the ``load_memory_variables({})``\n",
    "\n",
    "We can also decide to name this memory variable ourselves. Here's how we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62aafd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ''}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed95c1",
   "metadata": {},
   "source": [
    "#### Adding data into the memory\n",
    "\n",
    "To add data, we can add data about what the **user input** was or what **the AI(LLM's) response** was. Here's how we do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f86658cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"hello there!\")\n",
    "memory.chat_memory.add_ai_message(\"Hey, how can I assist you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cb3e42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human: hello there!\\nAI: Hey, how can I assist you today?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4732280",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"Can you tell me more aboout XYZ\")\n",
    "memory.chat_memory.add_ai_message(\"Sure, here's what you need to know about XYZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "808a571b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: hello there!\\nAI: Hey, how can I assist you today?\\nHuman: Can you tell me more aboout XYZ\\nAI: Sure, here's what you need to know about XYZ\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14135b",
   "metadata": {},
   "source": [
    "#### Returning Messages As A String Or A List Of Messages\n",
    "\n",
    "Messages can be returned as a string or a list of messages. It depends on where the messages are passed to. For LLMs, a simple string is best. For a ChatModel a list form is better. By default the messages are returned in form of a string. You can change this to get a list in return using ``return_message=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0037b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30084f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"hello there!\")\n",
    "memory.chat_memory.add_ai_message(\"Hey, how can I assist you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "771e4bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd8a75bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='hello there!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hey, how can I assist you today?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}).get(\"history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba0698",
   "metadata": {},
   "source": [
    "#### Keys Saved To Memory\n",
    " \n",
    "In some situations, chains (sequences of actions) can involve multiple inputs or outputs with different names. To manage this complexity, the system uses parameters called \"input_key\" and \"output_key\" associated with memory types. These parameters determine which specific input or output keys are stored in the chat message history. By default, these parameters are set to None, and if there's only one input/output key, it's automatically used. However, when there are multiple input/output keys, it's crucial to specify the particular key you want to use, ensuring clarity and control in managing the information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f69b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44631708",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "template = \"\"\"You are a friendly AI that knows all about cats\n",
    "\n",
    "Previous conversations: {chat_history}\n",
    "\n",
    "Human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Here we need to align the memory_key to align with the one used in the prompt template string above\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d813bc82",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly AI that knows all about cats\n",
      "\n",
      "Previous conversations: \n",
      "\n",
      "Human question: Hello there, tell me the best cat food\n",
      "Response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello there, tell me the best cat food',\n",
       " 'history': '',\n",
       " 'text': \" Hi there! The best cat food really depends on your cat's individual needs. Generally, you want to look for a food that is high in protein, low in carbs, and free of fillers. Also, make sure it is specifically designed for cats, as their nutritional needs are different than those of other animals.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we just pass in the `question` variables the other placeholder `chat_history` gets populated by memory\n",
    "chain({\"question\": \"Hello there, tell me the best cat food\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbbbc3",
   "metadata": {},
   "source": [
    "#### Using ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a541564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "300bf50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a friendly AI that knows all about cats\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# `return_messages` is set to True to get a list of messages since we are using a ChatModel\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b1b67bf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a friendly AI that knows all about cats\n",
      "Human: Hello there, tell me the best cat food\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello there, tell me the best cat food',\n",
       " 'chat_history': [HumanMessage(content='Hello there, tell me the best cat food', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello! When it comes to choosing the best cat food, there are a few factors to consider. It\\'s important to look for a cat food that is nutritionally balanced and meets the specific needs of your cat. Here are some key things to consider:\\n\\n1. High-quality ingredients: Look for cat foods that list high-quality protein sources, such as chicken, turkey, or fish, as the main ingredient. Avoid foods that contain a lot of fillers, artificial preservatives, or by-products.\\n\\n2. Complete and balanced nutrition: Ensure that the cat food you choose is labeled as \"complete and balanced\" by the Association of American Feed Control Officials (AAFCO). This means it provides all the necessary nutrients for your cat\\'s life stage, whether it\\'s a kitten, adult, or senior cat.\\n\\n3. Life stage and specific needs: Consider your cat\\'s age, activity level, and any specific dietary requirements or health conditions they may have. Some cats may benefit from specialized diets, such as those formulated for weight management, urinary health, or sensitive stomachs.\\n\\n4. Wet or dry food: Both wet and dry cat food have their advantages. Wet food can provide additional hydration and can be easier for some cats to eat, while dry food can help maintain dental health. A combination of both can be a good option.\\n\\n5. Consult your veterinarian: Your veterinarian can provide valuable insights and recommendations based on your cat\\'s individual needs and health history.\\n\\nRemember, what works best for one cat may not work for another, so it\\'s important to find the right cat food that suits your cat\\'s preferences and nutritional needs.', additional_kwargs={}, example=False)],\n",
       " 'text': 'Hello! When it comes to choosing the best cat food, there are a few factors to consider. It\\'s important to look for a cat food that is nutritionally balanced and meets the specific needs of your cat. Here are some key things to consider:\\n\\n1. High-quality ingredients: Look for cat foods that list high-quality protein sources, such as chicken, turkey, or fish, as the main ingredient. Avoid foods that contain a lot of fillers, artificial preservatives, or by-products.\\n\\n2. Complete and balanced nutrition: Ensure that the cat food you choose is labeled as \"complete and balanced\" by the Association of American Feed Control Officials (AAFCO). This means it provides all the necessary nutrients for your cat\\'s life stage, whether it\\'s a kitten, adult, or senior cat.\\n\\n3. Life stage and specific needs: Consider your cat\\'s age, activity level, and any specific dietary requirements or health conditions they may have. Some cats may benefit from specialized diets, such as those formulated for weight management, urinary health, or sensitive stomachs.\\n\\n4. Wet or dry food: Both wet and dry cat food have their advantages. Wet food can provide additional hydration and can be easier for some cats to eat, while dry food can help maintain dental health. A combination of both can be a good option.\\n\\n5. Consult your veterinarian: Your veterinarian can provide valuable insights and recommendations based on your cat\\'s individual needs and health history.\\n\\nRemember, what works best for one cat may not work for another, so it\\'s important to find the right cat food that suits your cat\\'s preferences and nutritional needs.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we just pass in the `question` variables the other placeholder `chat_history` gets populated by memory\n",
    "chain({\"question\": \"Hello there, tell me the best cat food\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ab0f9",
   "metadata": {},
   "source": [
    "## Chat Messages\n",
    "\n",
    "The ChatMessageHistory class is a fundamental tool in memory modules, used across various scenarios. It's a simple and efficient wrapper that offers easy methods to save and retrieve both human and AI messages. If you're handling memory independently from a sequence of actions, you can directly employ this class for better management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea0eec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4616005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi there', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hello, nice to hear from you', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Hi there\")\n",
    "history.add_ai_message(\"Hello, nice to hear from you\")\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b08ac8",
   "metadata": {},
   "source": [
    "## Types Of Memory\n",
    "\n",
    "LangChain provides us with different types of memory classes. Each memory class has it's own usage and parameters. Let's explore these different types of memory we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6adc02",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory\n",
    "\n",
    "This is used to store messages and then extract these messages in form of variables. We have taken a look at this type of memory so far. Lets make a closer look again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3308d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f61ca109",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c113847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi there\\nAI: Hello, nice to hear from you'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734859f",
   "metadata": {},
   "source": [
    "We can also get a list form as mentioned earlier. This is suitable for chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fb0d572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi there', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello, nice to hear from you', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdda25",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab4810c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047775a",
   "metadata": {},
   "source": [
    "### Conversation buffer window memory\n",
    "\n",
    "This is similar to the `conversation buffer` memory we looked as earlier on. The major difference being that the `window` name, meaning that only a list of the previous interactions are stored. Specifically the last `k` interactions. This helps to keep the memory buffer from going  large and also reduce token cost. Since we have less text being passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5500731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49ece468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Tell me about yourself\\nAI: Am an AI language model\\nHuman: Tell me about yourself, and what you enjoy doing\\nAI: Am an AI language model'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself, and what you enjoy doing\"}, {\"output\": \"Am an AI language model\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa0098",
   "metadata": {},
   "source": [
    "You can see the output of the above code will only keep track of the last to interactions, this means the last two inputs and thier respectice ouputs.\n",
    "\n",
    "We can also get a list form as mentioned earlier. This is suitable for `chat models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4467348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Tell me about yourself', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Tell me about yourself, and what you enjoy doing', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself, and what you enjoy doing\"}, {\"output\": \"Am an AI language model\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7dd7a",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b83d9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    # keep only the last 4 interactions in buffer memory\n",
    "    memory=ConversationBufferWindowMemory(k=4),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83dc06",
   "metadata": {},
   "source": [
    "### Entity Memory\n",
    "\n",
    "`Entity Memory` is a system that retains provided facts about certain entities within a conversation. It gathers details about these entities using a large language model (LLM) and gradually enhances its understanding of them using the same LLM over time. Essentially, it collects and stores information about specific subjects within the conversation, using a language model to both extract and expand its knowledge about those subjects. Using this type of memory is more costly as additional LLM calls will have to be made to gather details on an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a7dca85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79559dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationEntityMemory(llm=llm)\n",
    "\n",
    "_input = {\"input\": \"Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\"}\n",
    "\n",
    "memory.load_memory_variables(_input)\n",
    "\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" They must be the coolest kids in town\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce069723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\\nAI:  They must be the coolest kids in town\",\n",
       " 'entities': {'Johnson': 'Johnson is an older brother to Helen, adopted by her parents from different parents.'}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Johnson'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949c937",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2150f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb6738d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2bc11054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': '', 'Johnson': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's interesting. Can you tell me more about Helen and Johnson's relationship?\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00804b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents.',\n",
       " 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents.\"}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "677f3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents.', 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents.\", 'pack': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "AI:  That's interesting. Can you tell me more about Helen and Johnson's relationship?\n",
      "Last line:\n",
      "Human: They are both at the pack playing football\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like fun! What position does each of them play?'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are both at the park playing football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f38a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents, and is currently playing football at the park.', 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents. He is currently playing football at the park.\"}\n",
      "\n",
      "Current conversation:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "AI:  That's interesting. Can you tell me more about Helen and Johnson's relationship?\n",
      "Human: They are both at the pack playing football\n",
      "AI:  That sounds like fun! What position does each of them play?\n",
      "Last line:\n",
      "Human: They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! Do they have any other hobbies or interests they like to pursue together?\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "71eaad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted by her parents, and is currently playing football at the park as a defender, while Johnson plays as a mid-fielder.', 'Johnson': \"Johnson is an older brother to Helen, from different parents, and was adopted by Helen's parents. He is currently playing football at the park, with Helen playing as a defender and Johnson as a mid-fielder.\"}\n",
      "\n",
      "Current conversation:\n",
      "Human: Helen and Johnson are sisters and brothers from different parents. Johnson is older than Helen and was adopted by Helens' parents.\n",
      "AI:  That's interesting. Can you tell me more about Helen and Johnson's relationship?\n",
      "Human: They are both at the pack playing football\n",
      "AI:  That sounds like fun! What position does each of them play?\n",
      "Human: They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\n",
      "AI:  That's great! Do they have any other hobbies or interests they like to pursue together?\n",
      "Last line:\n",
      "Human: Yah, they also like to ride thier bikes in thier free time together doing races\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like a lot of fun! Do they ever compete against each other?'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Yah, they also like to ride thier bikes in thier free time together doing races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003d370",
   "metadata": {},
   "source": [
    "#### Inspecting Memory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d1f437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "50ba09e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted '\n",
      "          'by her parents, and is currently playing football at the park as a '\n",
      "          'defender, while Johnson plays as a mid-fielder. They also enjoy '\n",
      "          'riding bikes together in their free time and doing races.',\n",
      " 'Johnson': 'Johnson is an older brother to Helen, from different parents, and '\n",
      "            \"was adopted by Helen's parents. He is currently playing football \"\n",
      "            'at the park, with Helen playing as a defender and Johnson as a '\n",
      "            'mid-fielder, and they also enjoy riding their bikes together and '\n",
      "            'doing races in their free time.',\n",
      " 'pack': 'The pack is a location where Helen and Johnson play football.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cbfd9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Histon': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: They are both at the pack playing football\n",
      "AI:  That sounds like fun! What position does each of them play?\n",
      "Human: They are both in the same team. With Helen playing as a defender and Johnson as a mid-fielder\n",
      "AI:  That's great! Do they have any other hobbies or interests they like to pursue together?\n",
      "Human: Yah, they also like to ride thier bikes in thier free time together doing races\n",
      "AI:  That sounds like a lot of fun! Do they ever compete against each other?\n",
      "Last line:\n",
      "Human: John works part-time in a company called Histon. Histon makes clothes for you children.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like a great job! What kind of clothes does Histon make for children?'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"John works part-time in a company called Histon. Histon makes clothes for you children.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "91a96820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Helen': 'Helen is a sister to Johnson, who is older than her and was adopted '\n",
      "          'by her parents, and is currently playing football at the park as a '\n",
      "          'defender, while Johnson plays as a mid-fielder. They also enjoy '\n",
      "          'riding bikes together in their free time and doing races.',\n",
      " 'Histon': 'Histon is a company that makes clothes for children.',\n",
      " 'Johnson': 'Johnson is an older brother to Helen, from different parents, and '\n",
      "            \"was adopted by Helen's parents. He is currently playing football \"\n",
      "            'at the park, with Helen playing as a defender and Johnson as a '\n",
      "            'mid-fielder, and they also enjoy riding their bikes together and '\n",
      "            'doing races in their free time.',\n",
      " 'pack': 'The pack is a location where Helen and Johnson play football.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adce413",
   "metadata": {},
   "source": [
    "### Conversation Knowledge Graph Memory\n",
    "\n",
    "\n",
    "This memory variant employs a knowledge graph to reconstruct and represent memories. What's a knowledge graph?\n",
    "\n",
    "I aksed `ChatGPT` on what a knowledge graph is and this is the response I get back, `correct response` by the way ;)\n",
    "\n",
    "\"A knowledge graph is a structured representation of information that captures relationships between different concepts, entities, or data points. It's a way to organize and store data in a format that highlights how various pieces of information are connected to each other. Knowledge graphs consist of nodes (representing entities or concepts) and edges (representing relationships between nodes).\n",
    "\n",
    "In simpler terms, you can think of a knowledge graph as a map that shows not only individual locations (data points) but also the roads and paths (relationships) connecting those locations. This structure allows for more sophisticated and context-aware querying, reasoning, and analysis of data, making it easier to understand the connections and patterns within a dataset.\"\n",
    "\n",
    "**Basic terms**\n",
    "\n",
    "1. **Nodes ->** These are concepts, entitie or objects\n",
    "2. **Edges ->** These is the information used to create a relationship between the nodes\n",
    "\n",
    "**Semantic Enrichment**\n",
    "\n",
    "This is the use of NLP(Natural Language Processing) to identify nodes and create relationships between the nodes.\n",
    "\n",
    "**Example use cases**\n",
    "\n",
    "Semantic enrichment can be used with collected user data to come up with Knowledge Graphs that can better improve you recommendations on search engines and social media platforms\n",
    "\n",
    "For more infor on knowledge graphs, I'll recommend this [video](https://www.youtube.com/watch?v=PZBm7M0HGzw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47f012b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "943fdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "\n",
    "memory.save_context({\"input\": \"Can you remind Jack to pick his sister from school\"}, {\"output\": \"What is the name of Jack's sister and what school is she at?\"})\n",
    "memory.save_context({\"input\": \"Jack's sister is called Helen and she studeis at Histon Primary School\"}, {\"output\": \"okay\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0b037a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'On Helen: Helen studies at Histon Primary School.\\nOn Jack: Jack has a sister name. Jack sister attends school.'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"Who is Helen\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee1e7732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack', 'Helen', 'Histon Primary School']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also more modularly get current entities from a new message (will use previous messages as context.)\n",
    "memory.get_current_entities(\"Jack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a2b55",
   "metadata": {},
   "source": [
    "### Getting Knowledge Triplets\n",
    "\n",
    "Knowledge triplets, often represented in the form of a knowledge graph, are a way to encode information about relationships between entities. A knowledge triplet consists of three main components: a subject, a predicate, and an object. This format is also referred to as \"Subject-Predicate-Object\" (SPO). Here's what each component means:\n",
    "\n",
    "**Subject ->** This is the entity about which the information is being stated. It's the main focus of the triplet.\n",
    "\n",
    "**Predicate ->** The predicate describes the relationship between the subject and the object. It's like a verb that connects the subject and object and explains the nature of their connection.\n",
    "\n",
    "**Object ->** The object is the entity to which the subject is related through the predicate. It completes the statement by specifying what the relationship means.\n",
    "\n",
    "For example, let's say we have the triplet \"Jack | loves | magoes.\" In this case:\n",
    "\n",
    "1. **Subject ->** Jack\n",
    "2. **Predicate ->** loves\n",
    "3. **Object ->** mangoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "208ca1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='Jack', predicate='loves', object_='mangoes')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_knowledge_triplets(\"Jack loves mangoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d87c7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack', 'Helen', 'Histon Primary School']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_current_entities(\"Jack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aab2ef",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "85b553cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38245590",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "# the order of the input_variables does not matter\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "chain_with_kg = ConversationChain(\n",
    "llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "600734e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Jackson is the brother of Helen. Helen gets picked up daily from school by Jackson\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Ah, so Jackson is Helen's brother. That's nice that he takes the time to pick her up from school every day. Do you have any siblings, Human?\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_kg.predict(input=\"Jackson is the brother of Helen. Helen gets picked up daily from school by Jackson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f8fe6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Jackson: Jackson picks up Helen. Jackson is a character in story. Jackson is in love with woman named Helen. Jackson is very determined to win her heart. Jackson often puts himself in dangerous situations to help others. Jackson is a loyal friend and always has Helen's best interests in mind.\n",
      "On Helen: Helen gets picked up daily from school.\n",
      "\n",
      "Conversation:\n",
      "Human: What do you know about Helen\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I know that Helen gets picked up daily from school. I also know that she is the object of Jackson's affections and that he is very determined to win her heart. He often puts himself in dangerous situations to help others and is a loyal friend who always has her best interests in mind.\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_kg.predict(input=\"What do you know about Helen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c68b987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Jackson: Jackson is in love with Helen. Jackson is a character in story. Jackson is in love with woman named Helen. Jackson is very determined to win her heart. Jackson often puts himself in dangerous situations to help others. Jackson is a loyal friend and always has Helen's best interests in mind. Jackson is very determined to win Helen's heart.\n",
      "On Helen: Helen gets picked up daily from school.\n",
      "\n",
      "Conversation:\n",
      "Human: What is the relationship between Jackson and Helen\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Jackson and Helen are in love. Jackson is very determined to win Helen's heart and often puts himself in dangerous situations to help others. He is a loyal friend and always has Helen's best interests in mind.\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_kg.predict(input=\"What is the relationship between Jackson and Helen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c22d9",
   "metadata": {},
   "source": [
    "### Conversation Summary Memory\n",
    "\n",
    "The `ConversationSummaryMemory` is an advanced memory type that offers a more intricate way of storing information. This type of memory generates a condensed summary of ongoing conversations, making it beneficial for managing lengthy discussions. It continuously summarizes the conversation as it progresses and stores this summary in memory. This stored summary can then be integrated into prompts or processing chains. This memory type is particularly advantageous for lengthy discussions where using the entire message history in the prompt would consume too many tokens. In essence, it simplifies and streamlines the representation of conversation content for better efficiency and comprehension.\n",
    "\n",
    "This type of memory is an additional cost as a call to the LLM is made to summarize the data in memory, but saves more tokens in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812fed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello there\"}, {\"output\": \"Hello, how can I assist you today?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c409932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human greets the AI, to which the AI responds with a question asking how it can assist.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1d69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the history as a list of messages (this is useful if you are using this with a chat model).\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "memory.save_context({\"input\": \"Hello there\"}, {\"output\": \"Hello, how can I assist you today?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02c0465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='\\nThe human and AI greet each other, with the AI offering to assist the human.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db06b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello there', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also utilize the predict_new_summary method directly.\n",
    "messages = memory.chat_memory.messages\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e92fc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human greets the AI, which responds by asking how it can be of assistance.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ddaac2",
   "metadata": {},
   "source": [
    "If you possess messages beyond this class, you can effortlessly initiate the class using `ChatMessageHistory`. Upon loading, a summary will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2657ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hello there\")\n",
    "history.add_ai_message(\"Hello, how can I assist you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4abb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm=llm,\n",
    "    chat_memory=history,\n",
    "    return_messsages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d0a2a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human greets the AI, to which the AI responds by asking how it can assist them.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a8880",
   "metadata": {},
   "source": [
    "You have the choice to expedite the initialization process by utilizing a pre-generated summary. This allows you to bypass the need to recreate the summary, offering a faster initialization method by directly initializing with the existing summary. This saves additional LLM call to recreate a summary hence less on token cost as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced69a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm=llm,\n",
    "    buffer=\"The human greets the AI, to which the AI responds by asking how it can assist them.\",\n",
    "    chat_memory=history,\n",
    "    return_messsages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b73e27",
   "metadata": {},
   "source": [
    "### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c712b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56580c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How are you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f9f9911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human greets the AI, to which the AI responds with a friendly greeting and inquiry about the human's wellbeing.\n",
      "Human: Pretty good\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great to hear! How can I help you today?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Pretty good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7bd22ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"\\n\\nThe human greets the AI, to which the AI responds with a friendly greeting and inquiry about the human's wellbeing. The human replies that they are doing pretty good, to which the AI expresses their pleasure and asks how they can help.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a369fa3",
   "metadata": {},
   "source": [
    "### Conversation Summary Buffer Memory\n",
    "\n",
    "This combines two sets of memory types, the `conversation buffer window memory` and the `conversation summary`. It does two main things. Keep a certain number of previous interactions in the buffer memory as well as a summary of previous interactions that exceeded the amount of previous interaction we told it to have stored in memory. Unlike the `conversation buffer window memory` that delete previous chats that exceed the amount we instructed it to store in memory, the `conversation summary buffer memory` creates a summary of the previous interactions rather than delete them. Unlike the `conversation summary buffer memory`, instead of counting the number of interactions, this approach decides when to clear interactions based on the length of tokens used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "485ed2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greeted the AI and the AI responded warmly, saying it was nice to hear from them. The human then asked the AI to tell them about itself, to which the AI responded that it was an AI language model.\\nHuman: What you enjoy doing\\nAI: Am an AI language model, I help humans'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=20)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"What you enjoy doing\"}, {\"output\": \"Am an AI language model, I help humans\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3af0824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='\\nThe human greeted the AI, to which the AI responded with a friendly greeting. The human then asked the AI to tell them about itself, to which the AI responded that it is an AI language model.', additional_kwargs={}),\n",
       "  HumanMessage(content='What you enjoy doing', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model, I help humans', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get the history as a list of messages (this is useful if you are using this with a chat model).\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=20, return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"What you enjoy doing\"}, {\"output\": \"Am an AI language model, I help humans\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec04b9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. It enjoys helping humans, as it is an AI language model.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also utilize the predict_new_summary method directly.\n",
    "messages = memory.chat_memory.messages\n",
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7a17e",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc668e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=40)\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36959849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you today?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f89189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello there\n",
      "AI:  Hi there! How can I help you today?\n",
      "Human: What is the largest planet in the solar system?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The largest planet in the solar system is Jupiter. It has a diameter of 142,984 km, and it is about 11.2 times the size of Earth.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"What is the largest planet in the solar system?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22f735f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI, who responds with a friendly greeting and an offer to help. The human then asks what the largest planet in the solar system is.\n",
      "AI:  The largest planet in the solar system is Jupiter. It has a diameter of 142,984 km, and it is about 11.2 times the size of Earth.\n",
      "Human: Which position is it from the sun?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Jupiter is the fifth planet from the sun, located between Mars and Saturn.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Which position is it from the sun?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23c23004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI, who responds with a friendly greeting and an offer to help. The human then asks what the largest planet in the solar system is, to which the AI responds that Jupiter is the largest planet, with a diameter of 142,984 km and 11.2 times the size of Earth.\n",
      "Human: Which position is it from the sun?\n",
      "AI:  Jupiter is the fifth planet from the sun, located between Mars and Saturn.\n",
      "Human: Do you thing Mars is habitable\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Do you thing Mars is habitable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82c489b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: \\nThe human greets the AI, who responds with a friendly greeting and an offer to help. The human then asks what the largest planet in the solar system is, to which the AI responds that Jupiter is the largest planet, with a diameter of 142,984 km and 11.2 times the size of Earth, and is the fifth planet from the sun.\\nAI:  Jupiter is the fifth planet from the sun, located between Mars and Saturn.\\nHuman: Do you thing Mars is habitable\\nAI:  I'm sorry, I don't know.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a2fc6",
   "metadata": {},
   "source": [
    "### Conversation Token Buffer Memory\n",
    "\n",
    "`ConversationTokenBufferMemory` maintains a memory buffer for recent interactions, and it decides when to clear interactions based on the total token length instead of the count of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14f4b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"What you enjoy doing\"}, {\"output\": \"Am an AI language model, I help humans\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbdc842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Hello, nice to hear from you\\nHuman: Tell me about yourself\\nAI: Am an AI language model\\nHuman: What you enjoy doing\\nAI: Am an AI language model, I help humans'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01d11a",
   "metadata": {},
   "source": [
    "Becareful when using this memory class, because at times, it can clear the whole memory and have nothing inside of the memory in case the text it needs to store is more than the allowed `max_token_limit`. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8efe66eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"What you enjoy doing\"}, {\"output\": \"Am an AI language model, I help humans\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cf3eacc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [AIMessage(content='Hello, nice to hear from you', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Tell me about yourself', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What you enjoy doing', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Am an AI language model, I help humans', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get the history as a list of messages (this is useful if you are using this with a chat model).\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40, return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi there\"}, {\"output\": \"Hello, nice to hear from you\"})\n",
    "memory.save_context({\"input\": \"Tell me about yourself\"}, {\"output\": \"Am an AI language model\"})\n",
    "memory.save_context({\"input\": \"What you enjoy doing\"}, {\"output\": \"Am an AI language model, I help humans\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76dbd7",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "745ef2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.5)\n",
    "memory = ConversationTokenBufferMemory(llm=llm ,max_token_limit=40, return_message=True)\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ddcddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "239eac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: What is the largest start in the Milkyway galaxy\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The largest star in the Milky Way galaxy is the red supergiant star VY Canis Majoris. It is located in the constellation Canis Major and is estimated to have a radius of 1,420 to 2,100 times that of the Sun.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"What is the largest start in the Milkyway galaxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a751959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bede97",
   "metadata": {},
   "source": [
    "### Vector Store Retriever Memory\n",
    "\n",
    "The `VectorStoreRetrieverMemory` stores memories in a VectorDB and retrieves the most important documents (snippets of conversation) when queried(**queries the top-K most \"salient\" docs every time it is called**). Unlike other memory classes, it doesn't keep track of interaction order. This approach is helpful for referring to relevant information shared earlier in the conversation, enhancing conversation context and AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b090df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84e6e5",
   "metadata": {},
   "source": [
    "#### Initialize your VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37f0f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings(openai_api_key=openai_api_key).embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3d9de",
   "metadata": {},
   "source": [
    "#### Create your the VectorStoreRetrieverMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91bc1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever that will be passed to memory, K is the number of salient documents we want to retrieve each time\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=3))\n",
    "# create actual memory\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48c1ef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Python is my most loved programming language\n",
      "output: Interesting.\n",
      "input: Python is my most loved programming language\n",
      "output: Interesting.\n",
      "input: Python is my most loved programming language\n",
      "output: Interesting.\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"My favourite food is Pizza\"}, {\"output\": \"Nice to know\"})\n",
    "memory.save_context({\"input\": \"I like Programming\"}, {\"output\": \"Interesting, what programming language do you like most\"})\n",
    "memory.save_context({\"input\": \"Python is my most loved programming language\"}, {\"output\": \"Interesting.\"})\n",
    "\n",
    "print(memory.load_memory_variables({\"prompt\": \"Do you think I really love Python\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784b7da",
   "metadata": {},
   "source": [
    "#### Example Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0797b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Relevant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "(You do not need to use these pieces of information if not relevant)\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\", \"history\"], template=_DEFAULT_TEMPLATE)\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de92f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: What is my favorite programming language?\n",
      "response:  That's a great question. Do you have a favorite programming language?\n",
      "input: I like Programming\n",
      "output: Interesting, what programming language do you like most\n",
      "input: I like Programming\n",
      "output: Interesting, what programming language do you like most\n",
      "Human: What is my favorite programming language?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's a tough question. Do you have a particular programming language that you prefer?\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"What is my favorite programming language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93982742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: Python is my most loved programming language\n",
      "output: Interesting.\n",
      "input: Python is my most loved programming language\n",
      "output: Interesting.\n",
      "input: Python is my most loved programming language\n",
      "output: Interesting.\n",
      "Human: What is my favorite programming language is Python\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' It seems that your favorite programming language is Python.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"What is my favorite programming language is Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28d69aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: My favourite food is Pizza\n",
      "output: Nice to know\n",
      "input: What is my favorite programming language?\n",
      "response:  That's a great question. Do you have a favorite programming language?\n",
      "input: What is my favorite programming language?\n",
      "response:  That's a tough question. Do you have a particular programming language that you prefer?\n",
      "Human: What is my favorite food?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your favorite food is pizza.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"What is my favorite food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec024719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: I like Programming\n",
      "output: Interesting, what programming language do you like most\n",
      "input: I like Programming\n",
      "output: Interesting, what programming language do you like most\n",
      "input: I like Programming\n",
      "output: Interesting, what programming language do you like most\n",
      "Human: Do you think I like programming?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That is difficult to say. I don't have enough information to make an accurate assessment. However, from your response, it seems that you do like programming. What programming language do you like the most?\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"Do you think I like programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caacc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
