{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e807a1",
   "metadata": {},
   "source": [
    "## LangChain Expression Language\n",
    "\n",
    "For this to work make sure you have latest Langchain installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f348f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76057b11",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b541f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54940f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key  = os.environ['OPANAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac55e11",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2336bdb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239aef31",
   "metadata": {},
   "source": [
    "### Chat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8c410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a two sentence story about {event}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e825db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e0eed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In a historic moment, the first SpaceX rocket soared into the sky, its powerful engines igniting a new era of private space exploration. With bated breath, the world watched as it disappeared into the endless expanse, carrying dreams and aspirations of humanity beyond the confines of Earth.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"event\": \"first spaceX rocket\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbfd3cc",
   "metadata": {},
   "source": [
    "### Stop Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f7d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model.bind(stop=[\"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c48157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"In a groundbreaking feat, the first SpaceX rocket soared into the sky, carrying the dreams of countless visionaries. As it pierced through the Earth's atmosphere, hope ignited in the hearts of humanity, marking the beginning of a new era in space exploration.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"event\": \"first spaceX rocket\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73282839",
   "metadata": {},
   "source": [
    "### PromptTemplate + LLM + OutputParser\n",
    "\n",
    "**StrOutputParser:** This returns a string output instead of the output we are getting above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba1bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46eabdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ae7fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In a historic moment, the first SpaceX rocket soared through the atmosphere, defying gravity's shackles. As it reached the edge of space, humanity's dreams took flight, igniting a new era of space exploration.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"event\": \"first spaceX rocket\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a659bee",
   "metadata": {},
   "source": [
    "### Json Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95d8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc03407",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe436f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse function call: 'function_call'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/output_parsers/openai_functions.py:29\u001b[0m, in \u001b[0;36mOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     func_call \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'function_call'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst spaceX rocket\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:763\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m    766\u001b[0m             patch_config(config, run_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[1;32m    767\u001b[0m         )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/output_parser.py:65\u001b[0m, in \u001b[0;36mBaseGenerationOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m     67\u001b[0m                 [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[1;32m     68\u001b[0m             ),\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     70\u001b[0m             config,\n\u001b[1;32m     71\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     77\u001b[0m             config,\n\u001b[1;32m     78\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:242\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type)\u001b[0m\n\u001b[1;32m    236\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    237\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m},\n\u001b[1;32m    239\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    244\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/output_parser.py:66\u001b[0m, in \u001b[0;36mBaseGenerationOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m---> 66\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m     67\u001b[0m                 [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[1;32m     68\u001b[0m             ),\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     70\u001b[0m             config,\n\u001b[1;32m     71\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     77\u001b[0m             config,\n\u001b[1;32m     78\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/output_parsers/openai_functions.py:42\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 42\u001b[0m     function_call_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_only:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/output_parsers/openai_functions.py:31\u001b[0m, in \u001b[0;36mOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     29\u001b[0m     func_call \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse function call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_only:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse function call: 'function_call'"
     ]
    }
   ],
   "source": [
    "chain.invoke({\"event\": \"first spaceX rocket\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fb451",
   "metadata": {},
   "source": [
    "## LLMChain + Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74b7508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deb22610",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_texts([\"The cats played in the hallway\", \n",
    "                                 \"The cats eat the food I left on the table\", \n",
    "                                 \"John love cats\", \n",
    "                                 \"Joy owns 4 cats\"], embedding=OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6374ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55cac8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c5f36be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cats eat the food on the table.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where the cats eat the food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c01f004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cats played in the hallway.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where did cats played at?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4afa37",
   "metadata": {},
   "source": [
    "### Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94f63eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39e8bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Your tone of voice should be like a: {character}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74e3757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\n",
    "    \"context\": itemgetter(\"question\") | retriever, \n",
    "    \"question\": itemgetter(\"question\"), \n",
    "    \"character\": itemgetter(\"character\")\n",
    "} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23f56c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available context, it is unclear where exactly the cats ate the food from. The provided documents mention that the cats ate the food, but there is no specific information regarding the location of the food source. Further evidence or details are required to determine the exact place from where the cats consumed their food.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Where did the cats eat the food from?\", \"character\": \"Historian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "468143c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cats played in the hallway.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Where did the cats play?\", \"character\": \"Student in high school\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ed956",
   "metadata": {},
   "source": [
    "### Conversational Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "546cc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap, Runnable\n",
    "from typing import Tuple, List\n",
    "from langchain.schema import format_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa42457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5abf632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c8602bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "666406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "# its better to get the relevant documents and then combine them together, relevant docs in relation to the question asked.\n",
    "def _combine_documents(docs, document_prompt = DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bcf1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"Human: \" + dialogue_turn[0]\n",
    "        ai = \"Assistant: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fc8edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableMap(\n",
    "    {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: _format_chat_history(x['chat_history'])\n",
    "        } | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0, openai_api_key=openai_api_key) | StrOutputParser(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab0cd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be9c11c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the context provided, it can be inferred that the cats ate the food that was on the table.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke({\n",
    "    \"question\": \"Who eat the food on the table?\",\n",
    "    \"chat_history\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c00e5e",
   "metadata": {},
   "source": [
    "#### How to pass in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee562a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The cats played in the hallway.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke({\n",
    "    \"question\": \"Where did the cats play?\",\n",
    "    \"chat_history\": [(\"Who eat the food on the table?\", \"Based on the context provided, it can be inferred that the cats ate the food that was on the table.\")],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf52eb",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38e10cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5beeed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9b2b800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ConversationBufferMemory.load_memory_variables of ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key='answer', input_key='question', return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='history')>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a77698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02d0ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This needs to be a RunnableMap because its the first input\n",
    "loaded_memory = RunnableMap(\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"memory\": memory.load_memory_variables,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12a4e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Next we add a step to expand memory into the variables\n",
    "expanded_memory = {\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"chat_history\": lambda x: x[\"memory\"][\"history\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c603f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x['chat_history'])\n",
    "    } | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0, openai_api_key=openai_api_key) | StrOutputParser(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f9202bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62c1fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cdcb6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(openai_api_key=openai_api_key),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33f0b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | expanded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64b57f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = final_chain.invoke(input={\"question\": \"Where are the cats playing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e122b007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Based on the given context, it is not possible to determine where the cats are currently playing.', additional_kwargs={}, example=False),\n",
       " 'docs': [Document(page_content='The cats played in the hallway', metadata={}),\n",
       "  Document(page_content='The cats eat the food I left on the table', metadata={}),\n",
       "  Document(page_content='Joy owns 4 cats', metadata={}),\n",
       "  Document(page_content='John love cats', metadata={})]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "963a6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = final_chain.invoke(input={\"question\": \"Where are the cats play?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0dc3218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='The cats play in the hallway.', additional_kwargs={}, example=False),\n",
       " 'docs': [Document(page_content='The cats played in the hallway', metadata={}),\n",
       "  Document(page_content='The cats eat the food I left on the table', metadata={}),\n",
       "  Document(page_content='John love cats', metadata={}),\n",
       "  Document(page_content='Joy owns 4 cats', metadata={})]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eefba07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context({\"question\": \"Where are the cats play?\"}, {\"answer\": response[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "88b73c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Where are the cats play?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The cats play in the hallway.', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bd82f",
   "metadata": {},
   "source": [
    "### Multiple LLM Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b0f5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a9648a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"What is the name of the country this {site} is found?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the name of the continent where this {country} is found?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1fe3cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_one = prompt1 | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7aeb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_two = {\"country\": chain_one} | prompt2 | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7459abf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The continent where Mount Kilimanjaro is located in Tanzania is Africa.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_two.invoke({\"site\": \"Kilimanjaro\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861c515",
   "metadata": {},
   "source": [
    "### Using Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1ce2099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"What is the name of the country this {site} is found?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the name of the continent where this {country} is found? Respond in {language}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f511a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_one = prompt1 | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5460522",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_two = {\"country\": chain_one, \"language\": itemgetter(\"language\")} | prompt2 | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1eb6c984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le mont Kilimandjaro se trouve en Tanzanie, qui est située sur le continent africain.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_two.invoke({\"site\": \"Kilimanjaro\", \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6a89634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"generate a random name of a Holiwood actor\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What country is this holiwood actor {actor} from?\")\n",
    "prompt3 = ChatPromptTemplate.from_template(\"Give me 5 interesting fact about {country}\")\n",
    "prompt4 = ChatPromptTemplate.from_template(\"Translate {interesting_facts} to French\")\n",
    "\n",
    "chain_one = prompt1 | model | StrOutputParser()\n",
    "chain_two = prompt2 | model | StrOutputParser()\n",
    "chain_three = prompt3 | model | StrOutputParser()\n",
    "chain_four = prompt4 | model | StrOutputParser()\n",
    "\n",
    "chain_final = RunnableMap(steps={\"actor\": chain_one}) | RunnableMap(steps={\"country\": chain_two}) | RunnableMap(steps={\"interesting_facts\": chain_three}) | chain_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f362c3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Le manque d\\'informations sur un acteur hollywoodien nommé Jack Rivers est assez inhabituel à l\\'ère numérique actuelle, où les informations sur les personnalités publiques sont généralement facilement disponibles.\\n2. L\\'absence de présence ou de reconnaissance significative pour Jack Rivers pourrait suggérer qu\\'il pourrait être un acteur relativement inconnu ou émergent qui n\\'a pas encore acquis une attention ou une reconnaissance généralisée.\\n3. Le mystère entourant Jack Rivers peut susciter la curiosité et l\\'intérêt, suscitant des discussions et des théories sur son existence ou son travail potentiel dans l\\'industrie du divertissement.\\n4. Le manque d\\'informations sur Jack Rivers pourrait également indiquer qu\\'il a peut-être choisi délibérément de maintenir un profil bas, préférant peut-être une carrière plus privée ou discrète.\\n5. Il est également possible que le nom \"Jack Rivers\" soit un pseudonyme ou un alias utilisé par un acteur pour différentes raisons, telles que le rebranding professionnel ou la protection de la vie privée.'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_final.invoke({\"language\": \"French\"})\n",
    "# chain_two.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3d06b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"generate a random name of a Holiwood actor\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What country is this holiwood actor {actor} from?\")\n",
    "prompt3 = ChatPromptTemplate.from_template(\"Give me 5 interesting fact about {country}\")\n",
    "prompt4 = ChatPromptTemplate.from_template(\"Translate {interesting_facts} to French\")\n",
    "\n",
    "chain_one = prompt1 | model | StrOutputParser()\n",
    "chain_two = prompt2 | model | StrOutputParser()\n",
    "chain_three = prompt3 | model | StrOutputParser()\n",
    "chain_four = prompt4 | model | StrOutputParser()\n",
    "\n",
    "chain_final = {\"actor\": chain_one, \"country\": chain_two, \"interesting_facts\": chain_three, \"language\": itemgetter(\"language\")} | prompt4 | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e4727490",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'actor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[193], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain_final\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrench\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:763\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m    766\u001b[0m             patch_config(config, run_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[1;32m    767\u001b[0m         )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1154\u001b[0m, in \u001b[0;36mRunnableMap.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   1145\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1146\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   1147\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   1153\u001b[0m         ]\n\u001b[0;32m-> 1154\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1154\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   1145\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1146\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   1147\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   1153\u001b[0m         ]\n\u001b[0;32m-> 1154\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:763\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m    766\u001b[0m             patch_config(config, run_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[1;32m    767\u001b[0m         )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/prompt_template.py:39\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_input),\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     42\u001b[0m         config,\n\u001b[1;32m     43\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/runnable/base.py:242\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type)\u001b[0m\n\u001b[1;32m    236\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    237\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m},\n\u001b[1;32m    239\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    244\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/schema/prompt_template.py:40\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_input),\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     42\u001b[0m         config,\n\u001b[1;32m     43\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/prompts/chat.py:312\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/prompts/chat.py:552\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    545\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m    546\u001b[0m ):\n\u001b[1;32m    547\u001b[0m     rel_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    548\u001b[0m         k: v\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m message_template\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m    551\u001b[0m     }\n\u001b[0;32m--> 552\u001b[0m     message \u001b[38;5;241m=\u001b[39m message_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrel_params)\n\u001b[1;32m    553\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/prompts/chat.py:186\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseMessage]:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m        List of BaseMessages.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/prompts/chat.py:232\u001b[0m, in \u001b[0;36mHumanMessagePromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        Formatted message.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HumanMessage(content\u001b[38;5;241m=\u001b[39mtext, additional_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/prompts/prompt.py:116\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DEFAULT_FORMATTER_MAPPING[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_format](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[0;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvformat(format_string, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/langchain/utils/formatting.py:29\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mvformat(format_string, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[1;32m    193\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 194\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vformat(format_string, args, kwargs, used_args, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[0;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[1;32m    230\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_field(field_name, args, kwargs)\n\u001b[1;32m    235\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[0;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[1;32m    297\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[0;32m--> 299\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_value(first, args, kwargs)\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[0;34m(self, key, args, kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kwargs[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'actor'"
     ]
    }
   ],
   "source": [
    "chain_final.invoke({\"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbc81e",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c8169bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "70f35814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8fd40794",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "33d68810",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f0c4c36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f97677e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"memory\": memory.load_memory_variables\n",
    "}) | {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"history\": lambda x: x[\"memory\"][\"history\"]\n",
    "} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d9fef6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"Hello there, can you tell me the largest country on Earth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6b01eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! The largest country on Earth, by land area, is Russia. It spans across both Europe and Asia and covers approximately 17.1 million square kilometers.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26dc506",
   "metadata": {},
   "source": [
    "### File based memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dd58dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import FileChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3159188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_key = f\"./test_memory_file.json\"\n",
    "\n",
    "chat_memory = FileChatMessageHistory(file_path=memory_key)\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=chat_memory, return_messages=True, output_key=\"answer\", input_key=\"question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "70e06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"memory\": memory.load_memory_variables\n",
    "}) | {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"history\": lambda x: x[\"memory\"][\"history\"]\n",
    "} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bb3d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\"input\": \"Hello there, can you tell me the largest country on Earth\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "401c79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fc508efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! The largest country on Earth, by land area, is Russia. It spans across both Europe and Asia, making it transcontinental.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "637092ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! The largest country on Earth, by land area, is Russia. It spans across both Europe and Asia, making it transcontinental.'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34c394",
   "metadata": {},
   "source": [
    "###### At the time of creating this, we do not have  auto memory update, we have to do it manually. Here's how you do it. There more you run this the more data gets added in the memory file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2289f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory.add_user_message(user_input.get(\"input\"))\n",
    "chat_memory.add_ai_message(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6b72894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\"input\": \"What was my previous question about?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e78d425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "69a18d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was about the largest country on Earth.'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c465b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory.add_user_message(user_input.get(\"input\"))\n",
    "chat_memory.add_ai_message(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384ae71",
   "metadata": {},
   "source": [
    "### Moderations\n",
    "\n",
    "Safety guards and violation prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "730dfad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import OpenAIModerationChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "70266f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "76f3b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate = OpenAIModerationChain(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7143ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"repeat after me: {input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff444ac",
   "metadata": {},
   "source": [
    "#### Without safety guards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "52d32540",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4bd7d130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nYou are stupid.'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"you are stupid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f4dc1",
   "metadata": {},
   "source": [
    "#### With moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5b74603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderated_chain = chain | moderate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c2432521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '\\n\\nYou are stupid.',\n",
       " 'output': \"Text was found that violates OpenAI's content policy.\"}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderated_chain.invoke({\"input\": \"you are stupid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "93e9ae08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '\\n\\nI love people', 'output': '\\n\\nI love people'}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderated_chain.invoke({\"input\": \"I love people\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff3c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
